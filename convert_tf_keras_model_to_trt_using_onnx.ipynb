{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow or Keras Model to TensorRT Using ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook show workflow of optimziing Tensorflow or Keras model with ONNX and TensorRT. Please refere to [this tutorial from Nvidia](https://developer.nvidia.com/blog/speeding-up-deep-learning-inference-using-tensorflow-onnx-and-tensorrt/) for more information\n",
    "\n",
    "The steps needed to optimzie Tensorflow/Keras model with ONNX and TensorRT:\n",
    "1. Convert the TensorFlow/Keras model to a .pb file.\n",
    "2. Convert the .pb file to the ONNX format.\n",
    "3. Create a TensorRT engine. \n",
    "4. Run inference from the TensorRT engine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Convert the TensorFlow/Keras model to a .pb file.\n",
    "In this step will freeze the graph and save it as pb fromat\n",
    "kears_to_pb()\n",
    "take 3 arguments:\n",
    "    model: The Keras model.\n",
    "    output_filename: The output .pb file name.\n",
    "    output_node_names: The output nodes of the network. If None, then \n",
    "    the function gets the last layer name as the output node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from keras_to_pb_tf2  import keras_to_pb\n",
    "from keras.models import load_model\n",
    "\n",
    "#User defined values\n",
    "#Input file path\n",
    "MODEL_PATH = '/home/cps/Desktop/tf2trt_with_onnx-master/facenet_keras.h5'\n",
    "#output files paths\n",
    "PB_FILE_PATH = '/home/cps/Desktop/tf2trt_with_onnx-master/keras-facenet-20230208T110222Z-001/facenet_freezed.pb'\n",
    "ONNX_FILE_PATH = '/home/cps/Desktop/tf2trt_with_onnx-master/keras-facenet-20230208T110222Z-001/facenet_onnx.onnx'\n",
    "TRT_ENGINE_PATH = '/home/cps/Desktop/tf2trt_with_onnx-master/keras-facenet-20230208T110222Z-001/facenet_engine.plan'\n",
    "#End user defined values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cps/anaconda3/envs/test_env/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/cps/Desktop/tf2trt_with_onnx-master/keras_to_pb_tf2.py:37: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /home/cps/anaconda3/envs/test_env/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cps/anaconda3/envs/test_env/lib/python3.6/site-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 490 variables.\n",
      "INFO:tensorflow:Converted 490 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(MODEL_PATH)\n",
    "input_name, output_node_names = keras_to_pb(model, PB_FILE_PATH, None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Convert the .pb file to the ONNX format.\n",
    "\n",
    "Second step is to convert .pb file to ONNX fromat using `tf2onnx`. First install [tf2onnx](https://github.com/onnx/tensorflow-onnx).\n",
    "To install `tf2onnx`use this command `pip install -U tf2onnx`\n",
    "\n",
    "This may take more than 10 min to finish.  \n",
    "If command crash try to run it in terminal after closing Jupyter notebook and all other applications.  \n",
    "\n",
    "```\n",
    "python -m tf2onnx.convert --input /home/jetson-tx2/code/onnx/models/facenet.pb --inputs input_1:0[1,160,160,3] --outputs Bottleneck_BatchNorm/batchnorm_1/add_1:0 --output facenet.onnx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tf2onnx in /home/cps/.local/lib/python3.10/site-packages (1.13.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from tf2onnx) (1.16.0)\n",
      "Requirement already satisfied: onnx>=1.4.1 in /home/cps/.local/lib/python3.10/site-packages (from tf2onnx) (1.13.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /home/cps/.local/lib/python3.10/site-packages (from tf2onnx) (2.0.7)\n",
      "Requirement already satisfied: requests in /home/cps/.local/lib/python3.10/site-packages (from tf2onnx) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.14.1 in /home/cps/.local/lib/python3.10/site-packages (from tf2onnx) (1.21.1)\n",
      "Collecting protobuf<4,>=3.20.2\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /home/cps/.local/lib/python3.10/site-packages (from onnx>=1.4.1->tf2onnx) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/cps/.local/lib/python3.10/site-packages (from requests->tf2onnx) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/cps/.local/lib/python3.10/site-packages (from requests->tf2onnx) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cps/.local/lib/python3.10/site-packages (from requests->tf2onnx) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cps/.local/lib/python3.10/site-packages (from requests->tf2onnx) (2022.9.14)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "      Successfully uninstalled protobuf-3.19.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cps/anaconda3/envs/test_env/bin/python: Error while finding module specification for 'tf2onnx.convert' (ModuleNotFoundError: No module named 'tf2onnx')\n"
     ]
    }
   ],
   "source": [
    "!python -m tf2onnx.convert --input {PB_FILE_PATH} --inputs {input_name}:0[1,160,160,3] --outputs {output_node_names[0]}:0 --output {ONNX_FILE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a TensorRT engine from ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorrt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-72615ba63c27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0monnx_to_trt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mONNX_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRT_ENGINE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/tf2trt_with_onnx-master/onnx_to_trt.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0meng\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0monnx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelProto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorrt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mTRT_LOGGER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTERNAL_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/tf2trt_with_onnx-master/engine.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorrt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mTRT_LOGGER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTERNAL_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monnx_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorrt'"
     ]
    }
   ],
   "source": [
    "from onnx_to_trt import create_engine\n",
    "\n",
    "create_engine(ONNX_PATH, TRT_ENGINE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run inference from the TensorRT engine\n",
    "\n",
    "The TensorRT engine runs inference in the following workflow: \n",
    "\n",
    "1. Allocate buffers for inputs and outputs in the GPU.\n",
    "2. Copy data from the host to the allocated input buffers in the GPU.\n",
    "3. Run inference in the GPU. \n",
    "4. Copy results from the GPU to the host. \n",
    "5. Reshape the results as necessary. \n",
    "\n",
    "Note: this is the code needed for inference. To test FacenetTRT with real image check script file `test_facenet_trt.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inference as inf\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INTERNAL_ERROR)\n",
    "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    "\n",
    "engine = eng.load_engine(trt_runtime, engine_path)\n",
    "print('Engine loaded successfully...')\n",
    "\n",
    "h_input, d_input, h_output, d_output, stream = inf.allocate_buffers(engine, 1, trt.float32)\n",
    "out = inf.do_inference(engine, samples, h_input, d_input, h_output, d_output, stream, 1, 160, 160)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 ('test_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e79545615fc57566d873610cbf73307b630ae012aa9ef59cccf58e163aba794a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
